{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75UE4NqB_Fzo"
   },
   "source": [
    "### An encoding class for easy look up of the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "UvU_rzBmDaUj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "LdsH9VRk-yyL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self, symbols = None):\n",
    "        \n",
    "        #dictionary to map the vocabulary with a id to build matrix\n",
    "        #add \"UNK\" for unknown word to the initial mapping  \n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = []\n",
    "\n",
    "        if symbols:\n",
    "            for sym in symbols:\n",
    "                self.update(sym)\n",
    "\n",
    "    def update(self, tok):\n",
    "        \n",
    "        #takes as input a symbol and build the mapping if it doesnt exist\n",
    "        if tok not in self.word2idx:\n",
    "            self.word2idx[tok] = len(self.idx2word)\n",
    "            self.idx2word.append(tok)\n",
    "\n",
    "    def lookup(self, tok, update = False):\n",
    "        \n",
    "        #find tok id given the string, if the tok does not exist return the idx of \"UNK\"\n",
    "        if tok not in self.word2idx:\n",
    "            if update:\n",
    "                self.update(tok)\n",
    "                return self[tok]\n",
    "            return self.word2idx[\"<unk>\"]\n",
    "            \n",
    "        return self.word2idx[tok]\n",
    "    def rev_lookup(self, idx):\n",
    "        \n",
    "        #find the tok string given the id\n",
    "        return self.idx2word[idx]\n",
    "    \n",
    "    def __getitem__(self, symbol):\n",
    "        \n",
    "        #if the symbol does not exist we see it as unk\n",
    "        return self.lookup(symbol)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.idx2word)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbPKx8Ci_jY7"
   },
   "source": [
    "### Load the data\n",
    "Conll reader reads the conllu file and Data loader gonna load the data and feed the encoded data to the model(either for training or inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ocCUaAxr_TyV"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for reading and writing UD CONLL data\n",
    "\"\"\"\n",
    "CONLL_FIELDS = [\"token\", \"pos\", \"features\", \"deprel\"]\n",
    "MWE_TAGS     = [\"B\", \"I\"]  #B for begin , I for inside\n",
    "\n",
    "\n",
    "def readfile(filename, update = False, toks_vocab=Vocabulary([\"<unk>\", \"<bos>\", \"<eos>\"]), tags_vocab=Vocabulary([\"B_X\"])):\n",
    "    \"\"\"\n",
    "    function to read and encode the corpus at one pass \n",
    "    signature for train corpus : X_toks, Y_tags = readfile(\"corpus/train.conllu\", update=True)\n",
    "    signature for test corpus/ dev corpus:  X_test, Y_test = readfile(\"corpus/train.conllu\", update=True, vocabtoks_train, vocabtags_train)\n",
    "    \"\"\"\n",
    "\n",
    "    istream              = open(filename, encoding = \"utf-8\")\n",
    "    X_toks, Y_tags       = [], []\n",
    "    sent_toks, sent_tags = [], []\n",
    "\n",
    "    for line in istream:\n",
    "        line = line.strip()\n",
    "        if line and line[0] != \"#\":\n",
    "            try:\n",
    "                tokidx, token, lemma, upos, pos, features, headidx, deprel, extended, _ = line.split()\n",
    "\n",
    "            except ValueError:\n",
    "                pass\n",
    "            if tokidx == \"1\":\n",
    "                #beginning of sentence, add false toks  \n",
    "                sent_toks.append(toks_vocab[\"<bos>\"])\n",
    "                sent_tags.append(tags_vocab[\"B_X\"])\n",
    "            \n",
    "            #extract simple mwe tags\n",
    "            mwe_tag = lambda x: \"I\" if features.startswith(\"component\") else \"B\"\n",
    "            #extract tagging information\n",
    "            sent_toks.append(toks_vocab.lookup(tok = token, update = update))\n",
    "            sent_tags.append(tags_vocab.lookup(tok = mwe_tag(features) + \"_\" + upos, update = update))\n",
    "                \n",
    "        elif sent_toks:\n",
    "            #end of sentence, add  false tokens \n",
    "            sent_toks.append(toks_vocab[\"<eos>\"])\n",
    "            sent_tags.append(tags_vocab[\"B_X\"])\n",
    "            X_toks.append(sent_toks)\n",
    "            Y_tags.append(sent_tags)\n",
    "            sent_toks, sent_tags = [], []\n",
    "\n",
    "    istream.close()\n",
    "    #return the encoded data in list of list, the nested list represents the sentences\n",
    "    return X_toks, Y_tags,toks_vocab, tags_vocab\n",
    "# [{\"token1\": \"token\", \"multiword\": \"mwe\", \"mwe lemma\": \"mwe lemma\"}, {\"token2\": \"token\", \"multiword\": \"mwe\"}, {\"token3\": \"token\", \"multiword\": \"mwe\"}]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "X7Xt_y3rDaUo"
   },
   "outputs": [],
   "source": [
    "#build train vocab\n",
    "X_toks, Y_tags,toks_vocab, tags_vocab = readfile(\"corpus/train.conllu\", update=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MWEDataset (Dataset):\n",
    "\n",
    "    def __init__(self,datafilename = None, toks_vocab=Vocabulary([\"<unk>\", \"<bos>\", \"<eos>\"]), tags_vocab=Vocabulary([\"B_X\"]), isTrain = False, window_size = 0):\n",
    "        \"\"\"\n",
    "        take as input either the path to a conllu file or a list of tokens\n",
    "        we consider context size as the n preceding and n subsequent words in the text as the context for predicting the next word.\n",
    "        \"\"\"\n",
    "        super(MWEDataset, self).__init__()\n",
    "\n",
    "        self.toks_vocab, self.tags_vocab = toks_vocab, tags_vocab\n",
    "\n",
    "        self.Xtoks_IDs,self.Ytags_IDs, self.toks_vocab, self.tags_vocab = readfile(\"corpus/train.conllu\",\n",
    "                                                                                       update=isTrain, \n",
    "                                                                                       toks_vocab = toks_vocab, \n",
    "                                                                                       tags_vocab=tags_vocab)\n",
    "                                                                          \n",
    "\n",
    "     \n",
    "        print('token Vocab size',len(self.toks_vocab))\n",
    "        self.window_size  = window_size\n",
    "        self.data         = self.build_dataset(self.Xtoks_IDs,self.Ytags_IDs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def build_dataset(self,X_toks,Y_tags):\n",
    "        \"\"\"\n",
    "        build examples with contextual tokens as features\n",
    "        takes as input a nested list of encoded corpus, [sentences[tokens]]\n",
    "        return a list of examples with context window features\n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        for toks, tags in zip(X_toks, Y_tags):\n",
    "\n",
    "            toks = [self.toks_vocab[\"<bos>\"]]*self.window_size + toks + [self.toks_vocab[\"<eos>\"]]*self.window_size #3+3+3\n",
    "            \n",
    "            for i in range(self.window_size, len(toks)-self.window_size, 1): #3, 6, 1\n",
    "\n",
    "                examples.append((torch.tensor(toks[i - self.window_size : i + self.window_size+1]), torch.tensor(tags[i - self.window_size])))\n",
    "                #print(examples[-1])\n",
    "        return examples\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.data[idx]\n",
    "        \n",
    "    def as_strings(self,batch_tensor):\n",
    "        \"\"\"\n",
    "        Returns a string representation of a tensor of word indexes\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for line in batch_tensor.tolist():\n",
    "            out.append([self.tok_vocab.rev_lookup(idx) for idx in line])\n",
    "        return out\n",
    "\n",
    "    def get_loader(self, batch_size=1, num_workers=0, word_dropout=0., shuffle =False):\n",
    "        return DataLoader(self, batch_size=batch_size, num_workers=num_workers,shuffle = shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dJuLSsTNLKl"
   },
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1685174302234,
     "user": {
      "displayName": "Dan HOU",
      "userId": "05926720502448093799"
     },
     "user_tz": -120
    },
    "id": "JXOiRYCPDaUp",
    "outputId": "d44189d2-3be9-4d47-f506-f4467551220e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35694\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(toks_vocab))\n",
    "print(len(tags_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1685174316027,
     "user": {
      "displayName": "Dan HOU",
      "userId": "05926720502448093799"
     },
     "user_tz": -120
    },
    "id": "PLk3Q2G0DaUr",
    "outputId": "5cfcf7ca-2310-4e15-abfd-e3c165fe9b9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token Vocab size 35694\n",
      "token Vocab size 35694\n"
     ]
    }
   ],
   "source": [
    "corpuspath = \"corpus/train.conllu\"\n",
    "\n",
    "train_dataset = MWEDataset(\"corpus/train.conllu\",  isTrain = True,  window_size = 3)\n",
    "testset    = MWEDataset(\"corpus/test.conllu\", window_size = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsgjGw4vfq9_"
   },
   "source": [
    "# Model 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "rW-I3bYxkWP9"
   },
   "outputs": [],
   "source": [
    "class MweClassifer(nn.Module):\n",
    "    \n",
    "    def __init__(self, toks_vocab, tags_vocab, window_size = 0, emb_size=64, hidden_size=64, pretrainedw2v = None, drop_out = 0.):\n",
    "        \n",
    "        super(MweClassifer, self).__init__()\n",
    "        \n",
    "        self.word_embedding    = nn.Embedding(len(toks_vocab), emb_size)\n",
    "        self.window_size      = window_size\n",
    "        self.input_length      = 1 + window_size *2\n",
    "        self.toks_vocab        = toks_vocab\n",
    "        self.tags_vocab        = tags_vocab\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_size*self.input_length, hidden_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size*2),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size*2, len(tags_vocab)), #output # of classes\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        if pretrainedw2v:\n",
    "            self.word_embedding.weight.data.copy_(torch.from_numpy(self.pretrainedw2v_loader(pretrainedw2v).wv.vectors))\n",
    "        \n",
    "    @staticmethod\n",
    "    def pretrainedw2v_loader(self, path_to_pretrained = None):\n",
    "        if not path_to_pretrained:\n",
    "            # Download the pretrained French Word2Vec model https://fauconnier.github.io/#data\n",
    "            model_name = 'frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin.gz'\n",
    "            model = api.load(model_name)\n",
    "        else:\n",
    "            model = KeyedVectors.load_word2vec_format(path_to_pretrained, binary=True)\n",
    "        return model\n",
    "        \n",
    "    def forward(self, Xtoks_IDs):\n",
    "        b, seq = Xtoks_IDs.shape\n",
    "\n",
    "        input = self.word_embedding(Xtoks_IDs) # Batch, inputsize*emb_size\n",
    "        #print(input.shape) #B, window_size, emb_size\n",
    "        #input.view(b, -1) B, window_size * emb_size\n",
    "        return self.net(input.view(b, -1))           # tag_size = 3\n",
    "\n",
    "    def _init_weights(self):\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def train_model(self, train_data, test_data, epochs = 10, lr = 1e-3, batch_size = 10, device = \"cpu\", reg = None, split_train = 0.8):\n",
    "        \"\"\"\n",
    "        the train data is in form of nested lists: [sentences[tokens]]\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        #adaptive gradient descent, for every update lr is a function of the amount of change in the parameters\n",
    "        #optimizer   = torch.optim.Adam()\n",
    "        optimizer   = torch.optim.SGD(self.parameters(), lr = lr, momentum = 0.9)\n",
    "        loss_fnc    = nn.NLLLoss()\n",
    "        test_loader = test_data.get_loader(batch_size=batch_size)\n",
    "\n",
    "        train_loss = []\n",
    "\n",
    "        for e in range(epochs):\n",
    "            self.train()\n",
    "            #at every epochs we split the traindata into train set and dev set \n",
    "            num_train_examples = int(split_train*len(train_data))\n",
    "            trainset, validset = random_split(train_data, [num_train_examples, len(train_data) - num_train_examples])      \n",
    "            train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "            dev_loader   = DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            ep_loss = []\n",
    "            \n",
    "            for X_toks, Y_gold in tqdm(train_loader):\n",
    "                #print(x.shape)\n",
    "                optimizer.zero_grad()\n",
    "                logprobs = self.forward(X_toks)\n",
    "               \n",
    "                #print(y_hat.shape)\n",
    "                loss_value = loss_fnc(logprobs, Y_gold)\n",
    "                ep_loss.append(loss_value.item())\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "            loss = sum(ep_loss)/len(ep_loss)\n",
    "            train_loss.append(loss)\n",
    "            valid_loss = self.validate(dev_loader)\n",
    "            \n",
    "           \n",
    "            #print(\"Epoch %d | Mean train loss  %.4f | Mean dev loss %.4f\"%(e,loss, devloss) )\n",
    "            print(\"Epoch %d | Mean train loss  %.4f |  Mean dev loss  %.4f \"%(e,loss, valid_loss) )\n",
    "            print()\n",
    "        \n",
    "        average_precision, average_recall, average_f1_score = self.evaluation(test_loader)\n",
    "        print(\"Precision %.4f | Recall  %.4f |  F-score  %.4f \"%(average_precision, average_recall, average_f1_score) )\n",
    "\n",
    "\n",
    "    def validate(self, data_loader, device = \"cpu\"):\n",
    "        loss_fnc   = nn.NLLLoss()\n",
    "        loss_lst   = []\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_toks, Y_gold in tqdm(data_loader):\n",
    "                logprobs = self.forward(X_toks)\n",
    "               \n",
    "                loss = loss_fnc(logprobs, Y_gold)\n",
    "                loss_lst.append(loss)\n",
    "        return sum(loss_lst)/len(loss_lst)\n",
    "                \n",
    "            \n",
    "    def predict(self, string):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        pass\n",
    "    def evaluation(self, test_loader):\n",
    "        \"\"\"\n",
    "        evaluation the classifier with confusion matrix : precision recall and f-score\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        num_tags = len(self.tags_vocab)\n",
    "        print(num_tags)\n",
    "        TP   = torch.zeros(num_tags)\n",
    "        FP   = torch.zeros(num_tags)\n",
    "        FN   = torch.zeros(num_tags)\n",
    "        with torch.no_grad():\n",
    "            for X_toks, Y_golds in tqdm(test_loader):\n",
    "                logprobs              = self.forward(X_toks)\n",
    "                scores, predicted_IDs = torch.max(logprobs.data, dim = 1)\n",
    "                #convert tensor to np arrays\n",
    "                predicted_IDs = predicted_IDs.cpu().numpy()\n",
    "                Y_golds    = Y_golds.cpu().numpy()\n",
    "                for tag in range(num_tags):\n",
    "                    TP[tag] +=  ((predicted_IDs == tag) & (Y_golds == tag)).sum()\n",
    "                    FP[tag] += ((predicted_IDs == tag) & (Y_golds != tag)).sum()\n",
    "                    FN[tag] += ((predicted_IDs != tag) & (Y_golds == tag)).sum()\n",
    "        # Calculate precision, recall, and F1 score for each tag\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "       # Calculate average precision, recall, and F1 score\n",
    "        average_precision = torch.mean(precision)\n",
    "        average_recall = torch.mean(recall)\n",
    "        average_f1_score = torch.mean(f1_score)\n",
    "\n",
    "        return average_precision, average_recall, average_f1_score\n",
    "        \n",
    "    def load_model(self, modelpath):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "HKeJk2ewDaUs"
   },
   "outputs": [],
   "source": [
    "#define the hyperparameters\n",
    "batch_size    = 32\n",
    "window_size   =  3#left context and right context\n",
    "lr            = 1e-3\n",
    "device        = \"cpu\"\n",
    "epochs        = 50\n",
    "emb_size      = 64\n",
    "hidden_size   = 64\n",
    "nb_layers     = 2\n",
    "drop_out      = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token Vocab size 35694\n",
      "token Vocab size 35694\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MWEDataset(\"corpus/train.conllu\",  isTrain = True,  window_size = 3)\n",
    "testset    = MWEDataset(\"corpus/test.conllu\", window_size = 3)\n",
    "\n",
    "model = MweClassifer(toks_vocab   = train_dataset.toks_vocab,\n",
    "                     tags_vocab   = testset.tags_vocab, \n",
    "                     window_size  = 3, \n",
    "                     emb_size     = emb_size, \n",
    "                     hidden_size  = hidden_size, \n",
    "                     drop_out     = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7889/7889 [00:58<00:00, 134.27it/s]\n",
      "100%|█████████████████████████████████████| 1973/1973 [00:00<00:00, 2994.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Mean train loss  1.5375 |  Mean dev loss  1.1783 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7889/7889 [00:58<00:00, 134.80it/s]\n",
      "100%|█████████████████████████████████████| 1973/1973 [00:00<00:00, 2719.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Mean train loss  1.1193 |  Mean dev loss  1.0237 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 9861/9861 [00:36<00:00, 269.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision nan | Recall  0.2709 |  F-score  nan \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss = model.train_model(train_dataset,testset, epochs= 2, lr=1e-3, batch_size = batch_size, split_train=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "36mR4devDaUq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02773142185015094"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    tp, fn, fp = 0, 0, 0\n",
    "    for X, y_true in testset.get_loader(batch_size = 500):\n",
    "        #logits = model(X, X_deprel.zero_())\n",
    "        logits = model(X)\n",
    "        a, b, c = model.evaluation(logits, y_true)\n",
    "        tp+=a\n",
    "        fn+=b\n",
    "        fp+=c\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291898\n",
      "85\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "print(tp)\n",
    "print(fn)\n",
    "print(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997088871612388\n"
     ]
    }
   ],
   "source": [
    "print(tp/(fn+tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
